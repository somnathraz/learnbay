---
title: Regression techniques in Machine Learning
desc: "Learnbay one of the best instutute to learn data science course in India, so Enroll Now And Get Your Dream Job!"
slug: home
headerImg: "/blog/regression-01.png"
date: "may 29, 2022"
tag: [ Machine learning ]
category: "Machine learning"
author: "Admin"
position: "editor"
readTime: "7-8 mins"
h1: "Regression techniques in Machine Learning"
id: "regression-techniques-in-machine-learning"
tableData:
  [
   Regression techniques in Machine Learning,
    Additional points on Linear regressiont,
    Logistic regression,
  ]
---

#### Additional points on Linear regression:



1. There should be a linear relationship between the variables.
2. It is very sensitive to Outliers and can give a high variance and bias model.
3. The problem of occurring multi colinearity with multiple independent features


### Logistic regression:

It is used for classification problems with a linear dataset. In layman’s term, if the depending or target variable is in the binary form (1 0r 0), true or false, yes or no. It is better to decide whether an occurrence is possibly either success or failure.


<img src="/blog/regression-02.png" width="100%" /></img>



#### Additional point:



1. It is used for classification problems.
2. It does not require any relation between the dependent and independent features.
3. It can after by the outliers and can occur underfitting and overfishing.
4. It needs a large sample size to make the estimation more accurate.
5. It needs to avoid collinearity and multicollinearity.


### Polynomial regression:

The polynomial regression technique is used to execute a model that is suitable for handling non-linear separated data. It gives a curve that is best suited to data points, rather than a straight line.

The polynomial regression suits the least-squares form. The purpose of an analysis of regression to model the expected y value for the independent x of the dependent variable. 

The formula for this Y=  _β0+ β0x1+e_


<img src="/blog/regression-03.png" width="100%" /></img>


Additional  features: 

Look particularly for curve towards the ends to see if those shapes to patterns make logical sense. More polynomials can lead to weird extrapolation results. 


### Step-wise Regression:

It is used for statistical model fitting regression with predictive models. It is done automatically. 

The variable is supplemented or removed from the explanatory variable set at every step. The main approaches for the regression are reverse elimination and bidirectional elimination and step by step approaches. 

The formula for this: b = b(sxi/sy)

Additional points: 



1. This regression provides two things, the very first one is to add prediction for each steep and remove predictors fro each step.
2. It starts with the most significant predictor into the ML model and then adds features for each step.
3. The backward elimination starts with all the predictors into the model and then removes the least significant variable.


### Ridge Regression: 

It is a method that used when the dataset having multicollinearity which means, the independent variables are strongly related to each other. Although the least-squares estimates are unbiased in multicollinearity, So after adding the degree of bias to the regression, ridge regression can reduce the standard errors.


<img src="/blog/regression-04.png" width="100%" /></img>



#### Additional points:



1. In this regression, normality is not to be estimated the same as Least squares regression.
2. In this regression, the value could be varied but doesn’t come to zero.
3. This uses the l2 regularization method as it is also a regularization method.


### Lasso Regression:

Lasso is an abbreviation of the Least Absolute shrinkage and selection operator. This is similar to the ridge regression as it also analyzes the absolute size of the regression coefficients. And the additional features of that are it is capable of reducing the accuracy and variability of the coefficients of the Linear regression models.


<img src="/blog/regression-05.png" width="100%" /></img>


Additional points: 



1. Lasso regression shrinks the coefficients aero, which will help in feature selection for building a proper ML model.
2. It is also a regularization method that uses l1 regularization.
3. If there are many correlated features, it picks only one of them and shrinks it to the zero.

[Learnbay](https://www.learnbay.co/data-science-course/) provides industry accredited[ data science courses](https://www.learnbay.co/data-science-course/) in Bangalore. We understand the conjugation of technology in the field of Data science hence we offer significant courses like Machine learning, Regression techniques in Machine Learning,Tensor Flow, IBM Watson, Google Cloud platform, Tableau, Hadoop, time series, R, and Python. With authentic real-time industry projects. Students will be efficient by being certified by IBM. Around hundreds of students are placed in promising companies for data science roles. Choosing Learnbay you will reach the most aspiring job of present and future.

Learnbay data science course covers Data Science with Python, Artificial Intelligence with Python, Deep Learning using Tensor-Flow. These topics are covered and co-developed with IBM.
